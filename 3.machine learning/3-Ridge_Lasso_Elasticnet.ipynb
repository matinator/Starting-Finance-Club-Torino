{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defensive-liquid",
   "metadata": {},
   "source": [
    "# Ridge, Lasso and Elasticnet\n",
    "\n",
    "Ora che abbiamo capito come effettuare la regressione lineare anche con più di una variabile indipendente(MLR), è ora necessario vedere come applicare altri metodi lineari per ovviare ai problemi che possono influire sulle previsioni o sul training, ed essi sono:\n",
    "\n",
    "- collinearità: ovvero la possibilità di due o più feature di presentare una relazione di tipo lineare consistente nel dataset, questo in genere è un bene solo nel caso in cui sia perfetta, altrimenti può portare all'insordenza di problemi\n",
    "- dimensionalità: in alcuni casi potremmo avere un numero di feature molto grandi o addirittura maggiori del numero di samples al suo interno\n",
    "- overfitting  o underfitting: il modello non riesce ad estrapolare relazioni generali a causa della sua complessità maggiore o minore al problema.\n",
    "\n",
    "Per questo motivo, basandoci sempre sulla tecnica **OLS** sono state creati altre tecniche più robuste per la regressione, in genere queste tecniche che andremo ad analizzare si basano sulla __[Regolarizzazione](https://it.wikipedia.org/wiki/Regolarizzazione_(matematica))__ ovvero sull'**imposizione sulla formula di minimizzazione di un termine che penalizzi una specifica condizione legata al problema al fine di ridurne l'errore di generalizzazione**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-change",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
